<!doctype html><html class="no-js"><head><meta charset="utf-8"><title>SVBRDF Estimation</title><meta name="description" content=""><meta name="viewport" content="width=device-width">
<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
<link href="http://fonts.googleapis.com/css?family=Raleway:300,400,600" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="style.css">
        <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <!--<link rel="stylesheet" href="styles/main.37ab405b.css">-->
<body>
<!--[if lt IE 7]>
<p class="browsehappy">You are using an 
    <strong>outdated</strong> browser. Please 
    <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.
</p>
<![endif]-->
<div class="container">

    <nav class="navbar">
        <div class="container">
            <ul class="navbar-list">
                <li class="navbar-item">
                    <a class="navbar-link" href="#introduction">Intro</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#motivation">Motivation</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#fundamentals">Fundamentals</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#method">Method</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#evaluation">Evaluation</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#conclusion">Conclusion</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#downloads">Downloads</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#references">References</a>
                </li>
            </ul>
        </div>
    </nav>

    <section class="header" id="abstract">
        <h2 class="title">SVBRDF Estimation using a Physically-based Differentiable Renderer</h2>
        <h6>Project by Markus Andreas Worchel (<a href="mailto:m.worchel@campus.tu-berlin.de">m.worchel@campus.tu-berlin.de</a>)
        </h6>

        <div class="row">
<!--            <div class="one column category" style="text-align: center;">-->
                <img class="u-max-full-width" src="images/teaser_full.jpg">
                    <p>
                        Estimating scene parameters like geometry and material solely from images 
                        has been a widely researched topic, especially in the domain of computer vision. 
                        While there are well established methods for geometric reconstruction that are often 
                        able to leverage implicit knowledge about the camera setup, material reconstruction 
                        is a more ill-conditioned inverse problem. Recent advances in machine learning and 
                        computer graphics enable data driven approaches that use sophisticated light transport 
                        simulations for both, data generation as well as novel training objectives. 
                        So-called differentiable renderers allow using appearance-based objective functions for 
                        the training of neural networks by making the rendering operation differentiable with 
                        respect to arbitrary scene parameters.  
                        - Many methods use handcrafted renderers for the specific use case. 
                        - Scope of project: Integrate general renderer (pathtracer) into a pipeline for material reflectance estimation .
                        - Teaser image from the follow-up work <a href="#deschaintre_2019">[4]</a>.
                    </p>
        </div>
    </section>

    <div class="docs-section" id="motivation">
        <h3 class="section-heading">Motivation</h3>

        <p>
            Recent advances in computer graphics allow the simulation of virtual environments with quality closer
            to photo realism than ever before. This requires not only physically correct light transport
            simulations but also high quality assets that faithfully capture objects of the real world.
            Several industries including game development <a href="#brown_2016">[4]</a><a href="#bishop_2017">[5]</a>, movie production [NEEDCITATION] as well as projects concerned
            with cultural heritage [NEEDCITATION] rely on photogrammetry for asset acquisition.
            It allows reconstructing geometry and color information of real world objects
            only based on images, thus is a relatively light-weight capture process yielding
            high quality assets. While geometry reconstruction can be automated for the most parts due to the 
            availability of robust algorithms, appearance-related material properties of the surfaces 
            like their reflectance behavior often have to be manually defined by artists.
        </p>

        <p>
            Such photogrammetry workflows would greatly benefit from methods that provide automatic deduction of these 
            appearant material properties, especially if they support a light-weight capture process without expensive
            and difficult to deploy hardware. In this context, image-based approaches appear to be particularly interesting
            as they could be easily integrated into existing workflows.<br/>
            - Name research work focusing on material (reflectance) estimation<br/>
            - Name the specific paper and problem<br/>
            - Name the goal
        </p>

    </div>

    <div class="docs-section" id="fundamentals">
        <h3 class="section-heading">Material Model</h3>

        <div class="u-pull-right" style="text-align: center; width:300px; margin-left:20px">
            <h5 id="fig:svbrdf" class="docs-header">Figure 2</h5>
            <img class="u-max-full-width" src="images/fundamentals_svbrdf.png">
            <p>
                Visualization of the parameters of a SVBRDF. \(\mathbf{x}\) is a spatial position on the surface, 
                \(\mathbf{\omega}_i\) the direction of irradiance and \(\mathbf{\omega}_o\) the direction
                of reflected radiance. \(\mathbf{n}\) is the local normal.
            </p> 
        </div>

        <p>
            In computer graphics, surface reflectance properties are typically represented by a 
            bidirectional reflectance distribution function (BRDF) \( f_r(\mathbf{\omega}_i, \mathbf{\omega}_o) \).
            It defines the fraction of (differential) irradiance coming from a direction \(\mathbf{\omega}_i\) 
            that is reflected towards a direction \(\mathbf{\omega}_o\) <a href="#pharr_2016">[6]</a>. As most
            surfaces are not fully homogeneous, the BRDF is usually given as a function of surface position 
            \(f_r(\mathbf{x}, \mathbf{\omega}_i, \mathbf{\omega}_o)\), which is then also called spatially varying BRDF (SVBRDF).
            <a href="#fig:svbrdf">Figure 2</a> shows a visualization of the SVBRDF parameters. 
            Note that the directional quantities in this function are relative to the local coordinate system that is
            defined by the surface normal \(\mathbf{n}\). 
            Additionally, a physically correct BRDF fulfills two properties <a href="#pharr_2016">[6]</a>:
            <ol>
                <li>Helmholtz reciprocity: \(f_r(\mathbf{x}, \mathbf{\omega}_i, \mathbf{\omega}_o) = f_r(\mathbf{x}, \mathbf{\omega}_o, \mathbf{\omega}_i)\) for all \(\mathbf{\omega}_i, \mathbf{\omega}_o\)</li>
                <li>Energy Conservation: \(\int\limits_{\Omega} \, f_r(\mathbf{x}, \mathbf{\omega}_i, \mathbf{\omega}_o) \, \cos \theta_i \, d\mathbf{\omega}_i \leq 1, \) 
                    where \(\Omega\) is the hemisphere above \(\mathbf{x}\) and \(\theta_i\) the angle between \(\mathbf{\omega}_i\) and the surface normal \(\mathbf{n}\). 
                </li>
            </ol>
            Intuitively, the reciprocity states, that the reflectance properties are the same
            independent of the specific flow direction of light from one direction to the other.
            The energy conservation ensures that not more radiance is reflected from the surface than
            what is incident on it.
        </p>

        <p>
            While it is possible to represent a BRDF as a set of samples over the full parameter space,
            it is common to use simple and more intuitive models. The Cook-Torrance microfacet model <a href="#torrance_1982">[7]</a>
            is of particular interest for this project and will be the BRDF representation of choice.
            In this model, the BRDF consists of two terms
            \[ f_r(\mathbf{x}, \mathbf{\omega}_i, \mathbf{\omega}_o) = k_d f_d(\mathbf{x}) + k_s f_s(\mathbf{x}, \mathbf{\omega}_i, \mathbf{\omega}_o), \quad k_d + k_s = 1. \]
            The diffuse component \(f_d\) is independent of the directions and captures effects 
            such as internal scattering or surface interreflections while 
            the specular component \(f_s\) captures explicit reflectance effects without surface penetration.
            -- Surface consists of small differently oriented microfacets<br/>
            -- Isotropic specular reflection -> only relative difference between \(\mathbf{\omega}_i\) and \(\mathbf{\omega}_o\) matter<br/>
            - Four maps -> normal (in tangent space as variation from the macroscopic surface normal), diffuse albedo, roughness, specular albedo<br/>
            (- Flat surface)
        </p>
    </div>

    <div class="docs-section" id="method">
        <h3 class="section-heading">SVBRDF Estimation from Images</h3>

        <p>
            The main focus of this project is the work of Deschaintre et al. <a href="#3">[3]</a>, 
            a light-weight image-based method for capturing SVBRDFs of real world surfaces.
            Given a single flash-lit image of a flat surface, they are able to reconstruct its 
            Cook Torrance SVBRDF maps using a deep convolutional network. Thus, they cast material
            reflectance estimation as an image-to-image translation problem.
            Their follow-up work <a href="#deschaintre_2019">[4]</a> extends the idea by using multiple uncalibrated
            but aligned images of the same surface. While we initially planned to base the whole project on the multi-view approach,
            time constraints and lower overall complexity lead us to focus only on the single-view case. However, some ideas
            and the implementation reference are mainly borrowed from the multi-view approach.
        </p>

        <h5>Architecture</h5>

        <p>
            At the heart of the presented approach is a deep convolutional network that adopts the U-Net architecture [NEEDCITATION]
            which has shown to be suitable for other image-to-image translation problems. The input to the network is a conventional 
            three channel RGB image of the surface and the output is a nine channel image, where two channels represent
            the per-pixel normal (the z coordinate is inferred from the x and y coordinates), three channels represent
            the diffuse albedo, one channel represents the roughness and the last three channels represent the specular albedo.
            As this reflectance representation is conformant to the Cook Torrance model (see section <a href="#fundamentals">Material Model</a>),
            the approach intrinsically assumes that the surface can be represented by it.
        </p>

        <div class="row">
            <div class="column category" style="text-align: center;">
                <h5 id="fig:architecture_overview" class="docs-header">Figure 3</h5>
                <img class="u-max-full-width" src="images/architecture_overview.svg">
                <p>
                    TODO: Description (size of the blocks is not to scale)., TODO: Dimension of bottleneck, dimensions of global track layers
                </p>
            </div>
        </div>

        <p>
            The used U-Net architecture implements an encoder-decoder structure resembling the well-known hourglass shape.
            A coarse overview of the network is shown in <a href="#fig:architecture_overview">Figure 3</a>.
            The encoder part consists of eight convolutional layers with feature counts 64, 128, 256, 512, 512, 512, 512, 512
            that gradually reduce the spatial size of the image (using a stride of two) while increasing the number of features. 
            The bottleneck is followed by a decoder part which again consists of eight layers that perform spatial upsampling
            followed by two convolutions. The decoding layers have similar feature counts as the encoding layers, only in reversed order 
            with the last decoding layer having a feature count of nine to match the output format. In order to better reconstruct fine details
            that possibly have been lost in the bottleneck, each decoding layer is connected to the output of the same sized encoding layer
            through a skip connection. Leaky ReLUs with a negative slope of 0.2 serve as activation functions in these layers.
        </p>

        <p>
            In order to stabilize training, the network performs instance normalization between each convolutional layer
            and its preceeding activation function. However, the authors found that this normalization
            reduces the network's ability to maintain non-local information effectively. They showed that even
            a simple task such as predicting a constant colored diffuse albedo can impose a significant challenge
            on such an architecture. Their proposed solution is a second network parallel to the main encoder-decoder
            track that explicitely propagates global information.
            <a href="#fig:architecture_exchange">Figure 4</a> shows a detailed visualization of the 
            information exchange between the main track (in the encoding part) and the global track.
            The per-channel means that are subtracted by the instance normalization flow into
            the global track. They are concatenated with the previous feature vector,
            transformed by a fully connected layer and then pass through a SELU activation function.
            In the other direction, the previous feature vector is transformed by 
            another fully connected layer and then added
            to the output feature maps of the instance normalization.
            Using this second track, the network is able to propagate non-local information
            as part of a global feature vector.
        </p>

        <div class="row">
            <div class="column category" style="text-align: center;">
                <h5 id="fig:architecture_exchange" class="docs-header">Figure 4</h5>
                <img class="u-max-full-width" src="images/architecture_exchange.svg">
                <p>
                    TODO: Description.
                </p>
            </div>
        </div>

        <p>
            Note that specific details about the architecture of the single-view network are mainly extracted from the publicly
            available code of the multi-view approach. The single-view model (or at least a variation of it called 'Generator')
            is a subset of the multi-view model. There might be subtle differences between the model used
            in the single-view work and the single-view model presented here.
        </p>

        <h5>Data</h5>

        <p>
            Similar to other learning-based approaches, data is an integral part of this one aswell.
            In order to train the network presented in the previous section, several sample pairs
            of inputs and outputs, i.e., flash-lit images of the surface and their respective
            SVBRDF maps, are required. Since the latter is difficult to obtain for 
            real world surfaces in the necessary quantity, the authors chose to 
            use artificial training data that is representative for a large variety 
            of real materials. More specifically, they obtained a set of 1850 
            samples (SVBRDF maps) by selecting 155 materials from Allegorithmic Substance Share [NEEDCITATION]
            (a dataset of artist-generated procedural materials) and permutating 
            their important procedural parameters. Given an artificial material
            sample, the corresponding input image is generated by rendering
            a material patch placed in a virtual scene that contains one light source.
            The camera and light positions above the patch are randomly varied between 
            samples to capture different alignments of camera lense and flash
            position as well as camera field of views. Additionally,
            white balance and flash colors are varied between renderings.
            The final image is then artificially degenerated using Gaussian noise in an attempt
            to simulate sensor noise. <a href="#fig:materials">Figure 5</a> shows
            three material samples and their corresponding rendered input images.
        </p>

        <div class="row">
            <div class="column category" style="text-align: center;">
                <h5 id="fig:materials" class="docs-header">Figure 5</h5>
                <img class="u-max-full-width" src="images/mat1.png">
                <img class="u-max-full-width" src="images/mat2.png">
                <img class="u-max-full-width" src="images/mat3.png">
                <p>
                    TODO: Description.
                </p>
            </div>
        </div>

        <p>
            Since the number of materials obtained this way is still too<br/>
            - Material mixing (from multi-view)<br/>
        </p>

        <h5>Training Objective</h5>

        <p>
            - Rendering loss
        </p>

        <h5>Implementation</h5>

        <p>
            - PyTorch implementation<br/>
            - Validation of the architecture with TensorboardX (multi-view 'Generator' as reference)<br/>
        </p>

        <div class="row">
            <div class="one-half column category" style="text-align: center;">
                <h5 class="docs-header">Video Title</h5>
                <video class="u-max-full-width" controls>
                    <source src="videos/optimization_local_fixed.mp4" type="video/ogg">
                    Your browser does not support the video tag.
                </video>
                <p>Video description</p>
            </div>
            <div class="one-half column category" style="text-align: center;">
                <h5 class="docs-header">Video Title</h5>
                <video class="u-max-full-width" controls>
                    <source src="videos/optimization_local_flex.mp4" type="video/ogg">
                    Your browser does not support the video tag.
                </video>
                <p>Video description</p>
            </div>
        </div>

        <h5>Pathtracer Integration</h5>

    </div>

    <div class="docs-section" id="evaluation">
        <h3 class="section-heading">Evaluation</h3>

        - Comparison with my local renderer to reference renderer (reimplementation)

        - Comparison between my local renderer and pathtracer
        -- Training time
        -- Qualitative comparisons
        -- Quantitative comparisons (SSIM)
    </div>

    <div class="docs-section" id="conclusion">
        <h3 class="section-heading">Conclusion</h3>
    </div>

    <div class="docs-section" id="downloads">
        <h3 class="section-heading">Downloads</h3>

        <h5>Presentations</h5>
        
        <p>    
            Topic Presentation (<a href="slides/topic.pdf">pdf</a>, <a href="slides/topic.pptx">pptx</a>) <br>
            Technical Presentation* (<a href="slides/technical.pdf">pdf</a>, <a href="slides/technical.pptx">pptx</a>)<br>
            Intermediate Presentation* (<a href="slides/intermediate.pdf">pdf</a>, <a href="slides/intermediate.pptx">pptx</a>)<br>
            Final Presentation* (<a href="slides/final.pdf">pdf</a>, <a href="slides/final.pptx">pptx</a>)
        </p>

        <p>*videos only in pptx<p></p>

        <h5>Data and Code</h5>

        https://github.com/mworchel/svbrdf-estimation
    </div>

    <div class="docs-section" id="references">
        <h3 class="section-heading">References</h3>
        <ul class="popover-list">
            <li class="popover-item" id="1">
                <!--A link to this may look like: <a href="#1">[1]</a>-->
                [1] Einstein, A. et al., The problem of space, ether and the field in physics, <i>Man and the universe, 1947</i>.
            </li>
          
            <li class="popover-item" id="2">
                [2] MathJax, <a href="https://www.mathjax.org">https://www.mathjax.org</a>, last retrieved 09-2015
            </li>
            
            <li class="popover-item" id="3">
                [3] Deschaintre, V., Aittala, M., Durand, F., Drettakis, G., Bousseau, A., 2018, Single-Image SVBRDF Capture with a Rendering-Aware Deep Network.
            </li>

            <li class="popover-item" id="deschaintre_2019">
                [4] Deschaintre, V., Aittala, M., Durand, F., Drettakis, G., Bousseau, A., 2019, Flexible SVBRDF Capture with a Multi-Image Deep Network.
            </li>

            <li class="popover-item" id="brown_2016">
                [4] Brown, K., Hamilton, A., 2016, Photogrammetry and 'Star Wars Battlefront', GDC 2016, <a href="https://www.gdcvault.com/play/1022981/Photogrammetry-and-Star-Wars-Battlefront">https://www.gdcvault.com/play/1022981/Photogrammetry-and-Star-Wars-Battlefront</a>, last retrieved March 2020
            </li>

            <li class="popover-item" id="bishop_2017">
                [5] Bishop, L., Cowan, C., Jancosek, M., 2017,  	Photogrammetry for Games: Art, Technology and Pipeline Integration for Amazing Worlds, GDC 2017, <a href="https://www.gdcvault.com/play/1024340/Photogrammetry-for-Games-Art-Technology">https://www.gdcvault.com/play/1024340/Photogrammetry-for-Games-Art-Technology</a>, last retrieved March 2020
            </li>

            <li class="popover-item" id="pharr_2016">
                [6] Pharr, M., Jakob, W., Humphreys, G., 2016, Physically Based Rendering: From Theory to Implementation
            </li>

            <li class="popover-item" id="torrance_1982">
                [7] Torrance, K. E., Cook, R. L., 1982, A Reflectance Model for Computer Graphics
            </li>
        </ul>
    </div>

</div>

