{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook to recreate the figures on the website\n",
    "\n",
    "### Usage: Copy the notebook and the folder *webinput* into the *multiImage_pytorch* directory. Outputs are written to *./tmp/website*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "module_path = Path(\"./../multiImage_pytorch\").absolute()\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.append(str(module_path))\n",
    "    \n",
    "from dataset import SvbrdfDataset\n",
    "from utils import (enable_deterministic_random_engine, gamma_encode, \n",
    "                   read_image_tensor, write_image_tensor, write_image, \n",
    "                   unpack_svbrdf, pack_svbrdf, \n",
    "                   encode_as_unit_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_animated(output_path, renderer, svbrdf, flip_cam=True):\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for i, phi in enumerate(np.linspace(0.0, np.pi*2, 120, endpoint=False)):\n",
    "        cam       = Camera([0, 1, 1.4]) if flip_cam else Camera([0, -1, 1.4])\n",
    "        light     = Light([1.7 * np.cos(phi), 1.7 * np.sin(phi), 1.7], [30, 30, 30]) if flip_cam else Light([-1.7 * np.cos(phi), -1.7 * np.sin(phi), 1.7], [30, 30, 30])\n",
    "        scene     = Scene(cam, light)\n",
    "        rendering = renderer.render(scene, svbrdf).squeeze(0)\n",
    "        \n",
    "        mapping   = OrthoToPerspectiveMapping(cam, (512, 512))\n",
    "        rendering = mapping.apply(gamma_encode(rendering.clone().cpu().detach().permute(1, 2, 0)).numpy())\n",
    "\n",
    "        output_file_path = output_path.joinpath(\"{:d}.png\".format(i))\n",
    "        write_image(output_file_path, rendering)\n",
    "        \n",
    "def make_mat_sample(svbrdf, image, spacer_width=10, return_maps=False):\n",
    "    normals, diffuse, roughness, specular = unpack_svbrdf(svbrdf)\n",
    "    normals = encode_as_unit_interval(normals)\n",
    "    diffuse = gamma_encode(diffuse)\n",
    "    specular = gamma_encode(specular)\n",
    "    \n",
    "    spacer = torch.ones((3, normals.shape[-2], spacer_width))\n",
    "    \n",
    "    mat_sample = torch.cat([normals, spacer, diffuse, spacer, roughness, spacer, specular, spacer, spacer, spacer, spacer, gamma_encode(image)], dim=-1)\n",
    "    \n",
    "    if return_maps:\n",
    "        return mat_sample, normals, diffuse, roughness, specular\n",
    "    else:\n",
    "        return mat_sample\n",
    "    \n",
    "def make_video(input_path, output_path):\n",
    "    # Create a video from sequence of images\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-r\", \"30\", \"-i\", str(input_path / \"%d.png\"), \"-pix_fmt\", \"yuv420p\", str(output_path)]\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_path = Path(\"./../../documentation/website\")\n",
    "data_path = Path(\"./data/website\")\n",
    "tmp_path = Path(\"./tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SVBRDFs from a dataset\n",
    "\n",
    "In this experiment, we load SVBRDFs from a dataset on disk and rely on the generation of artificial input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_deterministic_random_engine(131)\n",
    "\n",
    "data = SvbrdfDataset(data_directory=str(data_path / \"materials\"), image_size=512, input_image_count=0, used_input_image_count=1, use_augmentation=True, scale_mode='crop')\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    mat_sample, normals, diffuse, roughness, specular = make_mat_sample(d['svbrdf'], d['inputs'][0], spacer_width=10, return_maps=True)\n",
    "    \n",
    "    write_image_tensor(website_path / \"images/mat{:d}.png\".format(i + 1), mat_sample)\n",
    "    \n",
    "    # Only write the individual maps for the first SVBRDF\n",
    "    if i == 0:\n",
    "        write_image_tensor(website_path / \"images/normals{:d}.png\".format(i + 1), normals)\n",
    "        write_image_tensor(website_path / \"images/diffuse{:d}.png\".format(i + 1), diffuse)\n",
    "        write_image_tensor(website_path / \"images/roughness{:d}.png\".format(i + 1), roughness)\n",
    "        write_image_tensor(website_path / \"images/specular{:d}.png\".format(i + 1), specular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material Mixing\n",
    "\n",
    "We pick two materials from the given dataset and mix them together according to the algorithm given in the paper. This example of mixing stone with metal demonstrates, that not all combinations produce plausible real world materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_deterministic_random_engine(113)\n",
    "\n",
    "data = SvbrdfDataset(data_directory=str(data_path / \"materials\"), image_size=512, input_image_count=0, used_input_image_count=1, use_augmentation=True, scale_mode='crop')\n",
    "\n",
    "_, svbrdf1 = data.read_sample(data.file_paths[0])\n",
    "_, svbrdf2 = data.read_sample(data.file_paths[2])\n",
    "svbrdf1x2 = data.mix(svbrdf1, svbrdf2, 0.3)\n",
    "\n",
    "rendering = data.render_inputs(svbrdf1x2, 1)[0]\n",
    "\n",
    "write_image_tensor(website_path / \"images/mat1xmat3.png\", make_mat_sample(svbrdf1x2, rendering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Ambient Lighting\n",
    "\n",
    "This experiment demonstrates that the absence of ambient lighting during input image generation can result in images with reduced information content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_deterministic_random_engine(313)\n",
    "\n",
    "data = SvbrdfDataset(data_directory=str(data_path / \"metal\"), image_size=512, input_image_count=0, used_input_image_count=1, use_augmentation=True, scale_mode='crop')\n",
    "write_image_tensor(website_path / \"images/metal_wo_ambient.png\", gamma_encode(data[0]['inputs'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Loss vs Rendering Loss\n",
    "\n",
    "Here, we just reorganize images from a figure in the paper that demonstrates the effects of using only L1 loss between the material maps versus using the rendering loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_deterministic_random_engine(313)\n",
    "\n",
    "in_directory  = data_path / \"l1vsrendering\"\n",
    "out_directory = website_path / \"images\"\n",
    "variants      = [\"gt\", \"l1\", \"rendering\"]\n",
    "\n",
    "for variant in variants:\n",
    "    normals   = read_image_tensor(str(in_directory / \"{}_normals.png\".format(variant)))\n",
    "    diffuse   = read_image_tensor(str(in_directory / \"{}_diffuse.png\".format(variant)))\n",
    "    roughness = read_image_tensor(str(in_directory / \"{}_roughness.png\".format(variant)))\n",
    "    specular  = read_image_tensor(str(in_directory / \"{}_specular.png\".format(variant)))\n",
    "    rendering = read_image_tensor(str(in_directory / \"{}_rendering.png\".format(variant)))\n",
    "    \n",
    "    # Spacer has half the size since these images are 256x256 instead of 512x512\n",
    "    spacer = torch.ones((3, normals.shape[-2], 5))\n",
    "    \n",
    "    mat_sample = torch.cat([normals, spacer, diffuse, spacer, roughness, spacer, specular, spacer, spacer, spacer, spacer, rendering], dim=-1)\n",
    "    \n",
    "    write_image_tensor(str(out_directory / \"l1vsrendering_{}.png\".format(variant)), mat_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization ortho to perspective mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Camera, Light, Scene\n",
    "from renderers import LocalRenderer, OrthoToPerspectiveMapping\n",
    "import pytweening as pt\n",
    "\n",
    "data = SvbrdfDataset(data_directory=str(data_path / \"materials\"), image_size=512, input_image_count=0, used_input_image_count=1, use_augmentation=True, scale_mode='crop')\n",
    "\n",
    "renderer = LocalRenderer()\n",
    "\n",
    "# Original: \n",
    "# cam    = Camera([-0.5, -1.8, 1.2])             \n",
    "# light  = Light([1, 1, 1.7], [30, 30, 30]) \n",
    "cam    = Camera([-0.5, -1.8, 0.2])\n",
    "light  = Light([1, 1, 0.7], [30, 30, 30]) \n",
    "scene  = Scene(cam, light)\n",
    "\n",
    "# Render\n",
    "image = torch.clamp(renderer.render(scene, data[2][\"svbrdf\"])[0].cpu().permute(1, 2, 0), min=0.0, max=1.0)\n",
    "image = gamma_encode(image).numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mapping = OrthoToPerspectiveMapping(cam, (512, 512))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "outdir = tmp_path / \"mapping\"\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "for i, t in enumerate(np.linspace(-1.0, 1.0, 120, endpoint=True)):\n",
    "    outdir_static = outdir / \"static\"\n",
    "    outdir_static.mkdir(parents=True, exist_ok=True)\n",
    "    # Previous easeInOutCubic\n",
    "    write_image(outdir_static / \"{:d}.png\".format(i), mapping.apply(image, pt.easeInOutQuint(np.abs(t))))\n",
    "    \n",
    "    phi = 2*np.pi*(t+1.0)*0.5\n",
    "    light_anim = Light([light.pos[0] * np.cos(phi), light.pos[1] * np.sin(phi), light.pos[2]], light.color)\n",
    "    scene_anim = Scene(cam, light_anim)\n",
    "    image_anim = torch.clamp(renderer.render(scene_anim, data[2][\"svbrdf\"])[0].cpu().permute(1, 2, 0), min=0.0, max=1.0)\n",
    "    image_anim = gamma_encode(image_anim).numpy()\n",
    "    \n",
    "    outdir_dynamic = outdir / \"dynamic\"\n",
    "    outdir_dynamic.mkdir(parents=True, exist_ok=True)\n",
    "    write_image(outdir_dynamic / \"{:d}.png\".format(i), mapping.apply(image_anim, pt.easeInOutQuint(np.abs(t))))\n",
    "    \n",
    "make_video(outdir_static, website_path / \"videos\" / \"patchsampling_static_light.mp4\")\n",
    "make_video(outdir_dynamic, website_path / \"videos\" / \"patchsampling_dynamic_light.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199) Loss: 0.049428\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from renderers import RednerRenderer, LocalRenderer\n",
    "from environment import Camera, Light, Scene\n",
    "\n",
    "data = SvbrdfDataset(data_directory=R\"./data/train\", \n",
    "                     image_size=256, scale_mode='crop', input_image_count=10, used_input_image_count=0, \n",
    "                     use_augmentation=False, mix_materials=False,\n",
    "                     no_svbrdf=False, is_linear=False)\n",
    "\n",
    "# Define loss function\n",
    "class FixedSceneLoss(torch.nn.Module):\n",
    "    def __init__(self, renderer):\n",
    "        super(FixedSceneLoss, self).__init__()\n",
    "        \n",
    "        self.renderer = renderer\n",
    "        \n",
    "        cam      = Camera([0, -1, 1.4])             # This configuration breaks the fixed evaluation!\n",
    "        light    = Light([1, 1, 1.7], [30, 30, 30]) #\n",
    "        #cam      = Camera([0, 0, 2])\n",
    "        #light    = Light([1, 1, 0.5], [20, 20, 20])\n",
    "        self.scene  = Scene(cam, light)\n",
    "        \n",
    "        self.target_rendering   = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if self.target_rendering is None:\n",
    "            self.target_rendering = self.renderer.render(self.scene, target)\n",
    "\n",
    "        estimated_rendering = self.renderer.render(self.scene, input)\n",
    "        \n",
    "        loss = torch.nn.functional.l1_loss(self.target_rendering, estimated_rendering)\n",
    "        \n",
    "        # Be conformant to the rendering loss and return \"multiple\" renderings for\n",
    "        # each batch instance \n",
    "        return loss, estimated_rendering.unsqueeze(0), self.target_rendering.unsqueeze(0)\n",
    "        \n",
    "\n",
    "output_dir             = Path(\"./tmp\")\n",
    "output_dir_optim_fixed = output_dir / \"optim_fixed\"\n",
    "output_dir_optim_fixed.mkdir(parents=True, exist_ok=True)\n",
    "output_dir_optim_fixed_svbrdf = output_dir_optim_fixed / \"svbrdf\"\n",
    "output_dir_optim_fixed_svbrdf.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup the loss function\n",
    "from losses import RenderingLoss\n",
    "renderer = LocalRenderer()\n",
    "#renderer = RednerRenderer()\n",
    "loss_function = RenderingLoss(renderer)\n",
    "#loss_function = FixedSceneLoss(renderer)\n",
    "\n",
    "svbrdf_t           = data[1]['svbrdf']\n",
    "n_t, d_t, r_t, s_t = unpack_svbrdf(svbrdf_t)\n",
    "\n",
    "optimization = \"normals\"\n",
    "\n",
    "# Estimated SVBRDF maps\n",
    "# Initialization with rand_like is important for a strong initial guess!\n",
    "n_e = torch.rand_like(n_t, requires_grad=True) if optimization == \"normals\" else n_t.clone()\n",
    "d_e = torch.rand_like(d_t, requires_grad=True) if optimization == \"diffuse\" else d_t.clone()\n",
    "r_e = torch.rand(r_t.shape[-2:], requires_grad=True) if optimization == \"roughness\" else r_t.clone()\n",
    "s_e = torch.rand_like(s_t, requires_grad=True) if optimization == \"specular\" else s_t.clone()\n",
    "\n",
    "params = []\n",
    "lr     = 5e-2\n",
    "if optimization == \"normals\":\n",
    "    params.append(n_e)\n",
    "    lr = 5e-2\n",
    "elif optimization == \"diffuse\":\n",
    "    params.append(d_e)\n",
    "    lr = 6e-3\n",
    "elif optimization == \"roughness\":\n",
    "    params.append(r_e)\n",
    "    lr = 6e-3\n",
    "elif optimization == \"specular\":    \n",
    "    params.append(s_e)\n",
    "    lr = 6e-3\n",
    "    \n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "losses  = []\n",
    "svbrdfs = []\n",
    "renderings_e = []\n",
    "renderings_t = []\n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    svbrdf_e = None\n",
    "    if optimization == \"normals\":\n",
    "        n_e_upperhemi  = torch.cat([n_e[:2], torch.clamp(n_e[2:], min=0.0001)])\n",
    "        n_e_normalized = n_e_upperhemi / (torch.sqrt(torch.sum(n_e_upperhemi**2, dim=0, keepdim=True)) + 0.001) #torch.nn.functional.normalize(n_e_upperhemi, dim=0)\n",
    "        svbrdf_e = pack_svbrdf(n_e_normalized, d_e, r_e, s_e)\n",
    "    elif optimization == \"diffuse\":\n",
    "        d_e_clamped = torch.clamp(d_e, min=0.001, max=1.0)\n",
    "        svbrdf_e = pack_svbrdf(n_e, d_e_clamped, r_e, s_e)\n",
    "    elif optimization == \"roughness\":\n",
    "        r_e_clamped = torch.clamp(r_e, min=0.001, max=1.0)\n",
    "        svbrdf_e = pack_svbrdf(n_e, d_e, torch.stack([r_e_clamped, r_e_clamped, r_e_clamped]), s_e)\n",
    "    elif optimization == \"specular\":\n",
    "        s_e_clamped = torch.clamp(s_e, min=0.001, max=1.0)\n",
    "        svbrdf_e = pack_svbrdf(n_e, d_e, r_e, s_e_clamped)\n",
    "        \n",
    "    loss, rendering_e, rendering_t = loss_function(svbrdf_e.unsqueeze(0), svbrdf_t.unsqueeze(0))\n",
    "    renderings_e.append(rendering_e[0].cpu().clone().detach())\n",
    "    renderings_t.append(rendering_t[0].cpu().clone().detach())\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    svbrdfs.append(svbrdf_e.cpu().clone().detach())\n",
    "    losses.append(loss.item())\n",
    "    print(\"({:03d}) Loss: {:f}\".format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "output_dir_optim_fixed_plots = output_dir_optim_fixed / \"plots\"\n",
    "output_dir_optim_fixed_plots.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "for i in range(len(losses)):\n",
    "    clear_output(wait=True)\n",
    "    dpi = mpl.rcParams['figure.dpi']\n",
    "    fig = plt.figure(figsize=(1620/dpi, 260/dpi))\n",
    "\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    #ax.yaxis.set_major_formatter(FormatStrFormatter('%0.01f'))\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    ax.set_ylim(bottom=0.0, top=max(losses))\n",
    "    ax.set_xlim(left=0, right=i+1)\n",
    "    ax.xaxis.set_tick_params(labelsize=20)\n",
    "    ax.yaxis.set_tick_params(labelsize=20)\n",
    "    plt.plot(losses[:i])\n",
    "    \n",
    "    plt.subplot(1, 6, 3)\n",
    "    n_t, d_t, r_t, s_t = unpack_svbrdf(svbrdf_t)\n",
    "    if optimization == \"normals\":\n",
    "        plt.imshow(encode_as_unit_interval(n_t).permute(1, 2, 0).numpy())\n",
    "    elif optimization == \"diffuse\":\n",
    "        plt.imshow(gamma_encode(d_t).permute(1, 2, 0).numpy())\n",
    "    elif optimization == \"roughness\":\n",
    "        plt.imshow(r_t.permute(1, 2, 0).numpy())\n",
    "    elif optimization == \"specular\":\n",
    "        plt.imshow(gamma_encode(s_t).permute(1, 2, 0).numpy())        \n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 6, 4)\n",
    "    plt.imshow((gamma_encode(torch.mean(renderings_t[i], dim=0)).detach().cpu().permute(1, 2, 0).numpy()))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 6, 5)\n",
    "    n_e, d_e, r_e, s_e = unpack_svbrdf(svbrdfs[i])\n",
    "    if optimization == \"normals\":\n",
    "        plt.imshow(encode_as_unit_interval(n_e).permute(1, 2, 0).numpy())\n",
    "    elif optimization == \"diffuse\":\n",
    "        plt.imshow(gamma_encode(d_e).permute(1, 2, 0).numpy())\n",
    "    elif optimization == \"roughness\":\n",
    "        plt.imshow(r_e.permute(1, 2, 0).numpy())\n",
    "    elif optimization == \"specular\":\n",
    "        plt.imshow(gamma_encode(s_e).permute(1, 2, 0).numpy())                \n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 6, 6)\n",
    "    plt.imshow((gamma_encode(torch.mean(renderings_e[i], dim=0)).detach().cpu().permute(1, 2, 0).numpy()))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.savefig(str(output_dir_optim_fixed_plots.joinpath(\"{:d}.png\".format(i))), bbox_inches='tight')\n",
    "    \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'cuda' and camera type 'CameraType.fullpatchsample' for redner\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n",
      "torch.Size([3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "from environment import Camera, Light, Scene\n",
    "from renderers import LocalRenderer, RednerRenderer, OrthoToPerspectiveMapping\n",
    "import numpy as np\n",
    "import pyredner\n",
    "\n",
    "\n",
    "        \n",
    "data = SvbrdfDataset(data_directory=\"./../../documentation/website/images/raw/materials\", image_size=512, input_image_count=0, used_input_image_count=1, use_augmentation=True, scale_mode='crop')\n",
    "\n",
    "outdir = Path(\"tmp/rendering\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "render_animated(outdir / \"local\", LocalRenderer(), data[2][\"svbrdf\"])\n",
    "render_animated(outdir / \"pathtracing\", RednerRenderer(), data[2][\"svbrdf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = [\"artificial\", \"real\"]\n",
    "methods = [\"reference\", \"local\", \"pathtracing\"]\n",
    "\n",
    "renderer = LocalRenderer()\n",
    "\n",
    "for origin in origins:\n",
    "    origin_dir = Path(\"./../../documentation/website/data/estimation\") / origin\n",
    "    \n",
    "    image_dataset = SvbrdfDataset(data_directory=origin_dir, image_size=256, input_image_count=1, used_input_image_count=1, no_svbrdf=True, is_linear=True, use_augmentation=True, scale_mode='crop')\n",
    "    \n",
    "    for i, image_data in enumerate(image_dataset):\n",
    "        output_dir = Path(\"./tmp/inputs\") / origin\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        write_image_tensor(output_dir / \"{}.png\".format(i), gamma_encode(image_data[\"inputs\"][0]))\n",
    "    \n",
    "    for method in methods:\n",
    "        method_dir = origin_dir / method\n",
    "        dataset = SvbrdfDataset(data_directory=method_dir, image_size=256, input_image_count=0, used_input_image_count=0, use_augmentation=True, scale_mode='crop')\n",
    "        \n",
    "        method_out_dir = Path(\"./tmp/renderings\") / origin / method\n",
    "        \n",
    "        for i, data in enumerate(dataset):\n",
    "            output_dir = method_out_dir / \"{}\".format(i)\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            render_animated(output_dir, renderer, data[\"svbrdf\"], flip_cam=False)\n",
    "            \n",
    "            # Create a video from \n",
    "            cmd = [\"ffmpeg\", \"-y\", \"-r\", \"30\", \"-i\", str(output_dir / \"%d.png\"), \"-pix_fmt\", \"yuv420p\", str(method_out_dir / \"{}.mp4\".format(i))]\n",
    "            subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods = [\"reference\", \"local\", \"pathtracing\"]\n",
    "\n",
    "data = SvbrdfDataset(data_directory=\"./../../documentation/website/data/estimation/artificial/pathtracing_broken_314\", image_size=256, input_image_count=0, used_input_image_count=0, use_augmentation=True, scale_mode='crop')\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    normals, diffuse, roughness, specular = unpack_svbrdf(d['svbrdf'])\n",
    "    normals  = encode_as_unit_interval(normals)\n",
    "    diffuse  = gamma_encode(diffuse)\n",
    "    specular = gamma_encode(specular)\n",
    "    \n",
    "    spacer = torch.ones((3, normals.shape[-2], 20))\n",
    "    \n",
    "    mat_sample = torch.cat([normals, spacer, diffuse, spacer, roughness, spacer, specular], dim=-1)\n",
    "    write_image_tensor(\"./tmp/mat{:d}.png\".format(i + 1), mat_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean time per mini batch:  0:00:02.469360\n",
      "mean time per mini batch:  0:00:19.754880\n",
      "mean time per epoch:  1:04:47.019576\n",
      "estimated training time:  11 days, 13:00:48.566640\n",
      "mean time per mini batch:  0:00:19.503548\n",
      "mean time per mini batch:  0:02:36.028384\n",
      "mean time per epoch:  8:31:40.534907\n",
      "estimated training time:  20 days, 18:54:41.735320\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "output_dir = Path(\"./tmp/losses\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "variations = [\"local\", \"pathtracing_broken_314\"]\n",
    "\n",
    "for variation in variations:\n",
    "    # Read tensorboard export\n",
    "    loss_path     = Path(\"./../../documentation/website/data/estimation\") / \"loss_{:s}.json\".format(variation)\n",
    "    segments_path = Path(\"./../../documentation/website/data/estimation\") / \"segments_{:s}.json\".format(variation)\n",
    "\n",
    "    loss = None\n",
    "    with open(loss_path) as file:\n",
    "        loss = json.load(file)\n",
    "\n",
    "    t = [datetime.fromtimestamp(d[0]) for d in loss]\n",
    "    x = [d[1] for d in loss]\n",
    "    y = [d[2] for d in loss]\n",
    "\n",
    "    mean_time_per_step = timedelta()\n",
    "    for i in range(0, len(x) - 1):\n",
    "        time_per_step = (t[i + 1] - t[i]) / (x[i + 1] - x[i])\n",
    "        mean_time_per_step = mean_time_per_step + (time_per_step - mean_time_per_step) / (i + 1)\n",
    "    \n",
    "    print(\"mean time per mini batch: \", mean_time_per_step)\n",
    "    print(\"mean time per mini batch: \", mean_time_per_step * 8)\n",
    "    print(\"mean time per epoch: \", (1590*0.99) * mean_time_per_step)\n",
    "    print(\"estimated training time: \", x[-1] * mean_time_per_step)\n",
    "    \n",
    "    dpi = mpl.rcParams['figure.dpi']\n",
    "    fig = plt.figure(figsize=(640/dpi, 480/dpi))\n",
    "\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    ax.xaxis.set_tick_params(labelsize=20)\n",
    "    ax.yaxis.set_tick_params(labelsize=20)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Step\", family='sans-serif', size=20)\n",
    "    plt.ylabel(\"Loss\", family='sans-serif', size=20)\n",
    "    \n",
    "    plt.savefig(str(Path(\"./tmp/losses\") / \"loss_{:s}.png\".format(variation)), bbox_inches='tight')\n",
    "    \n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda4f9e8ee5dfbe498093f52e44b6599c5c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
